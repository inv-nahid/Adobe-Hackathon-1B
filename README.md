# Adobe India Hackathon 2025 - Challenge 1B Solution

This repository contains the solution for Adobe India Hackathon 2025 Challenge 1B, which processes collections of PDF documents and generates structured JSON output with extracted sections, persona inference, and job-to-be-done analysis.

## Quick Start

### Prerequisites
- Docker installed and running
- Git (to clone the repository)

### Setup and Run

1. **Build the Docker image:**
   ```bash
   cd Adobe-Hackathon-1B
   docker build -t mysolutionname:somerandomidentifier .
   ```

## How to Run (Linux/macOS)

1. **Navigate to the collection directory:**
   ```bash
   cd "Adobe-Hackathon-1B/Collection 1"
   ```
2. **Create the output directory (required!):**
   ```bash
   mkdir -p output
   ```
   > This ensures Docker can mount the output directory and the container can write the output JSON file there.
3. **Run the Docker container:**
   ```bash
   docker run --rm -v "$(pwd):/app/input" -v "$(pwd)/output:/app/output" --network none mysolutionname:somerandomidentifier
   ```

- Place your PDFs and `challenge1b_input.json` in the collection directory.
- The output JSON will appear in the `output` directory inside the same collection.

**Note:** If you skip the `mkdir -p output` step, Docker may not mount the output directory correctly, and the output file may not be created as expected.


2. **Run for all collections:**
   ```bash
   # Collection 1 (Travel/France)
   mkdir -p "Collection 1/output"
   docker run --rm -v "$(pwd):/app/input" -v "$(pwd)/output:/app/output" --network none mysolutionname:somerandomidentifier

   # Collection 2 (Adobe Acrobat)
   mkdir -p "Collection 2/output"
   docker run --rm -v "$(pwd):/app/input" -v "$(pwd)/output:/app/output" --network none mysolutionname:somerandomidentifier

   # Collection 3 (Cooking/Recipes)
   mkdir -p "Collection 3/output"
   docker run --rm -v "$(pwd):/app/input" -v "$(pwd)/output:/app/output" --network none mysolutionname:somerandomidentifier
   ```

## Troubleshooting: Docker Paths on Windows (Git Bash/WSL)

**Note for Windows users:**

If you use Git Bash or WSL and encounter issues with Docker volume mounting (such as output not appearing in the expected folder, or strange folders like `output;C` being created), this is due to how Docker Desktop for Windows interprets Unix-style paths with spaces.

- **If you encounter issues on Windows with Git Bash/WSL:**
  Use the full Windows path in quotes:
  ```bash
  docker run --rm -v "E:/CS-PROJECTS/Adobe-India-Hackathon25/Adobe-Hackathon-1B/Collection 1:/app/input" -v "E:/CS-PROJECTS/Adobe-India-Hackathon25/Adobe-Hackathon-1B/Collection 1:/app/output" --network none mysolutionname:somerandomidentifier
  ```
  (Replace the path with your actual folder location.)

- **PowerShell users:**
  ```powershell
  docker run --rm -v "${PWD}:/app/input" -v "${PWD}/output:/app/output" --network none mysolutionname:somerandomidentifier
  ```

**This is a Docker Desktop for Windows quirk and will not affect Linux/macOS or PowerShell users.**

**The official command is correct for evaluation and will work for the judges.**

## üìÅ Project Structure

```
Adobe-Hackathon-1B/
‚îú‚îÄ‚îÄ Collection 1/
‚îÇ   ‚îú‚îÄ‚îÄ PDFs/                    # Input PDF files
‚îÇ   ‚îú‚îÄ‚îÄ challenge1b_input.json   # Input configuration
‚îÇ   ‚îî‚îÄ‚îÄ output/                  # Generated output
‚îÇ       ‚îî‚îÄ‚îÄ challenge1b_output.json
‚îú‚îÄ‚îÄ Collection 2/
‚îÇ   ‚îú‚îÄ‚îÄ PDFs/
‚îÇ   ‚îú‚îÄ‚îÄ challenge1b_input.json
‚îÇ   ‚îî‚îÄ‚îÄ output/
‚îÇ       ‚îî‚îÄ‚îÄ challenge1b_output.json
‚îú‚îÄ‚îÄ Collection 3/
‚îÇ   ‚îú‚îÄ‚îÄ PDFs/
‚îÇ   ‚îú‚îÄ‚îÄ challenge1b_input.json
‚îÇ   ‚îî‚îÄ‚îÄ output/
‚îÇ       ‚îî‚îÄ‚îÄ challenge1b_output.json
‚îú‚îÄ‚îÄ process_collection.py        # Main processing script
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ Dockerfile                   # Docker configuration
‚îî‚îÄ‚îÄ README.md                   # This file
```

## Technical Details

### Dependencies
- **PyMuPDF**: PDF text extraction and analysis
- **scikit-learn**: TF-IDF vectorization for keyword extraction
- **langdetect**: Language detection
- **psutil**: Memory and performance monitoring

### Processing Pipeline

1. **PDF Processing**: Extracts text, headings, and sections from all PDFs in the collection
2. **Language Detection**: Identifies the primary language of the content
3. **Keyword Extraction**: Uses TF-IDF to identify the most important terms
4. **Persona Inference**: Analyzes content to determine the target user persona
5. **Section Ranking**: Scores and ranks sections by relevance to the inferred persona
6. **Summary Generation**: Creates concise summaries of the most important sections

### Output Format

The solution generates `challenge1b_output.json` with the following structure:

```json
{
  "metadata": {
    "input_documents": ["file1.pdf", "file2.pdf", ...],
    "persona": "Inferred user persona",
    "job_to_be_done": "Primary task or goal",
    "processing_timestamp": "2025-07-28T01:05:42.174"
  },
  "extracted_sections": [
    {
      "document": "filename.pdf",
      "section_title": "Section heading or key text",
      "importance_rank": 1,
      "page_number": 3
    }
  ],
  "subsection_analysis": [
    {
      "document": "filename.pdf",
      "refined_text": "Summarized content",
      "page_number": 3
    }
  ]
}
```

## Performance Metrics

### Collection 1 (Travel/France)
- **Processing Time**: ~1.6 seconds
- **Memory Usage**: ~70 MB
- **Documents**: 7 PDFs
- **Sections Extracted**: 447 total

### Collection 2 (Adobe Acrobat)
- **Processing Time**: ~8.6 seconds
- **Memory Usage**: ~93 MB
- **Documents**: 15 PDFs
- **Sections Extracted**: 1,633 total

### Collection 3 (Cooking/Recipes)
- **Processing Time**: ~3.6 seconds
- **Memory Usage**: ~75 MB
- **Documents**: 9 PDFs
- **Sections Extracted**: 3,683 total

## üê≥ Docker Commands

### For Linux/macOS (Bash):
```bash
docker run --rm -v "$(pwd)/Collection 1:/app/input" -v "$(pwd)/Collection 1/output:/app/output" --network none mysolutionname:somerandomidentifier
```

### For Windows (PowerShell):
```powershell
docker run --rm -v "${PWD}/Collection 1:/app/input" -v "${PWD}/Collection 1/output:/app/output" --network none mysolutionname:somerandomidentifier
```

### Alternative: Rename directories to remove spaces
If you encounter issues with spaces in directory names, you can rename the directories:
```bash
# Rename directories to remove spaces
mv "Collection 1" Collection1
mv "Collection 2" Collection2
mv "Collection 3" Collection3

# Then use the simpler command format
docker run --rm -v $(pwd)/Collection1:/app/input -v $(pwd)/Collection1/output:/app/output --network none mysolutionname:somerandomidentifier
```

## Key Features

### Multilingual Support
- Automatic language detection
- Support for CJK (Chinese, Japanese, Korean) characters
- RTL (Right-to-Left) language support

### Intelligent Analysis
- **Persona Detection**: Automatically infers user personas like "Travel Planner", "AI Specialist", etc.
- **Job-to-be-Done**: Identifies the primary task or goal from content analysis
- **Section Ranking**: Uses semantic analysis to rank sections by importance

### Robust Processing
- Handles various PDF formats and structures
- Error handling for corrupted or unreadable files
- Memory-efficient processing for large document collections

## Testing

The solution has been tested with:
- ‚úÖ Collection 1: Travel planning documents (France)
- ‚úÖ Collection 2: Adobe Acrobat learning materials
- ‚úÖ Collection 3: Cooking and recipe collections

All collections process successfully and generate valid JSON output files.

## Notes

- The solution uses `--network none` for security and isolation
- Output directories are created automatically if they don't exist
- Processing time varies based on document complexity and size
- Memory usage is optimized for large document collections

## Expected Output

After running the Docker commands, you should find:
- `Collection 1/output/challenge1b_output.json`
- `Collection 2/output/challenge1b_output.json`
- `Collection 3/output/challenge1b_output.json`

Each file contains structured analysis of the respective collection's content. 